{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "150e4834",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81dbb278",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c09a36e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4452c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_batch = ChatGoogleGenerativeAI(model = \"gemini-2.5-flash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f02c9ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: What is LangChain\n",
      "Answer: **LangChain** is an open-source framework designed to simplify the development of applications powered by large language models (LLMs). It provides a structured way to connect LLMs with external data sources, computational tools, and other components, enabling developers to build more complex and capable LLM-driven applications than would be possible by simply interacting with an LLM API directly.\n",
      "\n",
      "Think of it as an **orchestration layer** for LLMs.\n",
      "\n",
      "---\n",
      "\n",
      "### Why LangChain? (The Problem It Solves)\n",
      "\n",
      "While LLMs are incredibly powerful, they have inherent limitations when used in isolation:\n",
      "\n",
      "1.  **Lack of Real-time Information:** LLMs are trained on a fixed dataset and don't inherently know about current events or proprietary business data.\n",
      "2.  **Inability to Perform Actions:** They can generate text, but they can't browse the web, query a database, send an email, or perform calculations.\n",
      "3.  **Complexity of Multi-step Interactions:** Building an application that requires multiple LLM calls, conditional logic, and integration with external systems becomes very cumbersome and boilerplate-heavy.\n",
      "4.  **Managing Context and Memory:** Maintaining conversation history (memory) across turns is crucial for chatbots but not natively handled by stateless LLM APIs.\n",
      "5.  **Prompt Engineering:** Crafting effective prompts for complex tasks can be challenging.\n",
      "\n",
      "LangChain addresses these challenges by providing abstractions and tools that make it easier to build sophisticated LLM applications.\n",
      "\n",
      "---\n",
      "\n",
      "### What Does LangChain Do? (Core Functionality)\n",
      "\n",
      "LangChain's primary goal is to empower LLMs to:\n",
      "\n",
      "*   **Connect to Data Sources:** Integrate with databases, APIs, documents, and other external knowledge bases (e.g., using vector stores for retrieval-augmented generation).\n",
      "*   **Interact with Tools:** Enable LLMs to use external tools like search engines, calculators, code interpreters, or custom APIs to perform actions or get up-to-date information.\n",
      "*   **Manage Conversation State (Memory):** Keep track of past interactions to provide context-aware responses.\n",
      "*   **Orchestrate Complex Workflows:** Chain together multiple LLM calls and other components in a logical sequence to achieve a specific goal.\n",
      "*   **Build \"Agents\":** Create LLM-powered entities that can dynamically decide which tools to use and in what order, based on the user's input.\n",
      "\n",
      "---\n",
      "\n",
      "### Key Concepts and Modules in LangChain\n",
      "\n",
      "LangChain is built around several modular components:\n",
      "\n",
      "1.  **Models:**\n",
      "    *   **LLMs:** Interfaces to various large language models (e.g., OpenAI's GPT models, Google's PaLM/Gemini, Anthropic's Claude, Hugging Face models).\n",
      "    *   **Chat Models:** Optimized interfaces for conversational LLMs.\n",
      "    *   **Embeddings:** Tools to create numerical representations (embeddings) of text, crucial for semantic search and retrieval.\n",
      "\n",
      "2.  **Prompts:**\n",
      "    *   **Prompt Templates:** Standardized ways to construct prompts for LLMs, including input variables and formatting instructions.\n",
      "    *   **Output Parsers:** Structures to extract and parse specific information from the LLM's text output into a usable format (e.g., JSON, Pydantic objects).\n",
      "\n",
      "3.  **Chains:**\n",
      "    *   Sequences of calls to LLMs or other utilities. Chains allow you to combine different components to achieve a specific task.\n",
      "    *   **Examples:** `LLMChain` (basic LLM call), `SequentialChain` (multiple steps in order), `RetrievalQAChain` (fetch documents then answer question).\n",
      "\n",
      "4.  **Retrieval:**\n",
      "    *   **Document Loaders:** Tools to load data from various sources (PDFs, websites, databases).\n",
      "    *   **Text Splitters:** Utilities to break large documents into smaller, manageable chunks.\n",
      "    *   **Vector Stores:** Integrations with databases that store text embeddings (e.g., Chroma, FAISS, Pinecone) for efficient semantic search.\n",
      "    *   **Retrievers:** Components that fetch relevant documents from a vector store based on a query.\n",
      "\n",
      "5.  **Memory:**\n",
      "    *   Mechanisms to persist state between calls of a chain or agent. This is essential for building conversational applications.\n",
      "    *   **Examples:** `ConversationBufferMemory` (stores all messages), `ConversationSummaryMemory` (summarizes past conversation).\n",
      "\n",
      "6.  **Agents:**\n",
      "    *   The most powerful feature. Agents use an LLM as a \"reasoning engine\" to decide which `Tools` to use and in what order to accomplish a goal.\n",
      "    *   They follow a \"thought, action, observation\" loop:\n",
      "        1.  **Thought:** LLM decides what to do next.\n",
      "        2.  **Action:** LLM selects a `Tool` and its inputs.\n",
      "        3.  **Observation:** The result of running the `Tool`.\n",
      "        4.  Repeat until the task is complete.\n",
      "\n",
      "7.  **Tools:**\n",
      "    *   Functions that an agent can invoke to interact with the outside world.\n",
      "    *   **Examples:** `SerpAPIWrapper` (for Google search), `Calculator`, `PythonREPLTool` (code execution), custom APIs.\n",
      "\n",
      "---\n",
      "\n",
      "### How It Works (Simplified Example)\n",
      "\n",
      "Imagine you want to build a chatbot that can answer questions about your company's internal documents and also look up current stock prices.\n",
      "\n",
      "1.  **User Input:** \"What's the policy on remote work, and what's Apple's stock price today?\"\n",
      "2.  **Agent (LLM as Brain):** LangChain's Agent, powered by an LLM, receives this query. It analyzes the query and decides it needs two pieces of information.\n",
      "3.  **Tool 1 (Internal Docs Retriever):** The Agent identifies that \"remote work policy\" requires accessing internal documents. It uses a `RetrievalQAChain` (which uses a vector store of your documents) as a tool.\n",
      "4.  **Tool 2 (Stock Price API):** The Agent identifies \"Apple's stock price\" requires real-time data. It uses a `SerpAPIWrapper` (or a custom stock API tool) to get this information.\n",
      "5.  **Combine & Respond:** LangChain collects the results from both tools. The Agent's LLM then synthesizes this information into a coherent, natural language response back to the user.\n",
      "\n",
      "---\n",
      "\n",
      "### Benefits of Using LangChain\n",
      "\n",
      "*   **Rapid Prototyping:** Quickly build and test complex LLM applications.\n",
      "*   **Modularity and Reusability:** Components are designed to be interchangeable and reusable.\n",
      "*   **Extensibility:** Easily integrate with new LLMs, data sources, and custom tools.\n",
      "*   **Reduced Boilerplate:** Abstracts away much of the repetitive code needed for LLM interactions.\n",
      "*   **Community and Ecosystem:** A large and active community contributes to its development and provides support.\n",
      "*   **Language Support:** Available in Python and JavaScript/TypeScript.\n",
      "\n",
      "---\n",
      "\n",
      "### Limitations and Considerations\n",
      "\n",
      "*   **Complexity:** While it simplifies LLM app development, LangChain itself has a steep learning curve due to its many modules and abstractions.\n",
      "*   **Overhead:** For very simple LLM calls, using LangChain might introduce unnecessary complexity.\n",
      "*   **Debugging:** Debugging agentic behavior can be challenging as the LLM's decision-making process isn't always transparent.\n",
      "*   **Rapid Evolution:** The framework is under very active development, meaning APIs and best practices can change frequently.\n",
      "\n",
      "---\n",
      "\n",
      "In essence, LangChain acts as a **powerful middleware** that bridges the gap between the raw capabilities of Large Language Models and the real-world data and actions required to build truly intelligent and useful applications.\n",
      "\n",
      "Question: What is LangGraph\n",
      "Answer: **LangGraph** is a library built on top of LangChain that allows you to build **stateful, multi-actor applications with Large Language Models (LLMs) by modeling their logic as a graph.**\n",
      "\n",
      "Think of it as a way to create highly complex and dynamic \"flowcharts\" for your LLM applications, where the flow isn't just linear but can branch, loop, and incorporate multiple independent \"actors\" (like different LLMs, tools, or even human input).\n",
      "\n",
      "Here's a breakdown of what that means:\n",
      "\n",
      "1.  **Graphs, Not Just Chains:**\n",
      "    *   Traditional LangChain focuses on \"chains,\" which are often linear sequences of operations (e.g., \"Prompt -> LLM -> Output Parser\").\n",
      "    *   LangGraph takes this a step further by using a **graph structure (nodes and edges)**. This allows for:\n",
      "        *   **Non-linear logic:** Branching based on conditions.\n",
      "        *   **Loops:** Repeating steps until a condition is met (e.g., \"try again,\" \"refine answer\").\n",
      "        *   **Parallel execution:** (Though often managed by the state, not true parallel threads in the graph itself).\n",
      "\n",
      "2.  **Stateful Execution:**\n",
      "    *   This is a core differentiator. Unlike many stateless chains, LangGraph maintains a **mutable state** that is passed between nodes.\n",
      "    *   Each node receives the current state, performs its operation, and then returns an update to the state. This allows the application to remember context, accumulate information, and make decisions based on past interactions.\n",
      "    *   This statefulness is crucial for building robust agents, multi-turn conversations, and complex workflows that require memory.\n",
      "\n",
      "3.  **Multi-Actor Applications:**\n",
      "    *   The \"actors\" in a LangGraph application can be various components:\n",
      "        *   **LLMs:** Different LLMs for different tasks (e.g., one for planning, one for content generation).\n",
      "        *   **Tools:** External APIs, databases, code interpreters.\n",
      "        *   **Custom Functions:** Any Python function you define.\n",
      "        *   **Humans:** For human-in-the-loop processes.\n",
      "    *   LangGraph orchestrates how these different actors interact and pass control (and state) among themselves.\n",
      "\n",
      "4.  **Key Concepts in LangGraph:**\n",
      "    *   **Nodes:** These are the individual steps or computational units in your graph. A node could be an LLM call, a tool invocation, a data processing step, or a conditional logic check.\n",
      "    *   **Edges:** These define the transitions between nodes.\n",
      "        *   **Default Edges:** Simply move from one node to the next.\n",
      "        *   **Conditional Edges:** The most powerful feature. Based on the output of a node (or the current state), you can define multiple possible \"next\" nodes. This is how you implement branching and looping.\n",
      "    *   **Graph State:** The central data structure that holds all the relevant information for the current execution of the graph. It's updated by each node.\n",
      "    *   **Compiled Graph:** LangGraph compiles your defined graph structure into an executable `Runnable` (from LangChain), making it efficient.\n",
      "\n",
      "5.  **Why Use LangGraph? (Benefits)**\n",
      "    *   **Handling Complexity:** Ideal for building sophisticated AI agents that require planning, reflection, self-correction, and iterative processes.\n",
      "    *   **Robustness:** More resilient to errors or unexpected outputs, as you can define fallback paths or retry mechanisms.\n",
      "    *   **Control Flow:** Gives developers fine-grained control over the application's logic, allowing for complex decision-making.\n",
      "    *   **Agentic Behavior:** Naturally supports the development of autonomous agents that can react to environments and achieve goals over multiple steps.\n",
      "    *   **Debugging:** The graph visualization can make complex flows easier to understand and debug.\n",
      "\n",
      "**In essence, LangGraph empowers you to move beyond simple linear chains and build truly dynamic, intelligent, and state-aware LLM applications that can handle real-world complexities.** It's particularly useful for advanced AI agents, multi-turn chatbots, and automated reasoning systems.\n",
      "\n",
      "Question: What is LangSmith\n",
      "Answer: **LangSmith** is an end-to-end platform developed by the creators of LangChain, designed specifically for **developing, debugging, testing, and monitoring Large Language Model (LLM) applications** throughout their lifecycle.\n",
      "\n",
      "Think of it as an MLOps platform, but tailored for the unique challenges of building and deploying LLM-powered systems.\n",
      "\n",
      "Here's a breakdown of what LangSmith is and why it's used:\n",
      "\n",
      "### What Problem Does LangSmith Solve?\n",
      "\n",
      "Developing LLM applications (like chatbots, agents, RAG systems) is complex:\n",
      "*   **Black Box Nature:** It's hard to see what's happening inside a multi-step LLM chain or agent.\n",
      "*   **Non-Deterministic Outputs:** LLMs can give different answers to the same prompt.\n",
      "*   **Debugging Challenges:** Pinpointing where an error occurred (e.g., wrong prompt, model hallucination, tool failure) is difficult.\n",
      "*   **Evaluation:** How do you objectively measure the quality, accuracy, or helpfulness of an LLM's output?\n",
      "*   **Production Monitoring:** How do you ensure your LLM app performs well in the wild, manage costs, and identify regressions?\n",
      "*   **Iteration:** How do you systematically improve your prompts, models, and chains?\n",
      "\n",
      "LangSmith provides the tools to address these challenges.\n",
      "\n",
      "### Key Features of LangSmith\n",
      "\n",
      "1.  **Observability & Tracing:**\n",
      "    *   **Visualizing Runs:** It records and visualizes every step of an LLM call, chain, or agent interaction. You can see inputs, outputs, intermediate thoughts, tool calls, and final responses.\n",
      "    *   **Detailed Metrics:** Provides insights into latency, cost (token usage), and success/failure rates for each component.\n",
      "    *   **Debugging:** Allows you to \"replay\" a run, inspect payloads, and pinpoint exactly where an issue (e.g., an incorrect prompt, a failed API call, a hallucination) occurred within a complex system.\n",
      "\n",
      "2.  **Testing & Evaluation:**\n",
      "    *   **Dataset Management:** Create and manage datasets of test cases (inputs and expected outputs) to evaluate your LLM application systematically.\n",
      "    *   **Automated Evaluation:** Run your application against these datasets and use various evaluators (e.g., LLM-as-a-judge, rule-based, or custom metrics) to automatically score the outputs.\n",
      "    *   **Human-in-the-Loop Feedback:** Collect human feedback on specific runs or outputs to refine your application.\n",
      "    *   **Comparison:** Compare different versions of your application (e.g., different prompts, models, chain architectures) side-by-side to determine which performs better.\n",
      "\n",
      "3.  **Monitoring & Production:**\n",
      "    *   **Performance Tracking:** Monitor the performance of your deployed LLM applications in real-time, tracking key metrics like latency, error rates, and token usage.\n",
      "    *   **Data Collection:** Collect production data (user queries, model responses) to identify common failure modes, improve prompts, or fine-tune models.\n",
      "    *   **A/B Testing:** Facilitate A/B testing of different LLM configurations in production.\n",
      "\n",
      "4.  **Prompt Management:**\n",
      "    *   Version control and manage your prompts, making it easier to iterate and deploy changes.\n",
      "\n",
      "### Relationship to LangChain\n",
      "\n",
      "LangSmith is built by the same team that develops LangChain and integrates seamlessly with LangChain applications. While LangChain provides the framework and tools to *build* LLM applications (chains, agents, retrievers, etc.), LangSmith provides the platform to *observe, test, and manage* those applications throughout their lifecycle, especially as they move towards production.\n",
      "\n",
      "### Who Uses LangSmith?\n",
      "\n",
      "*   **LLM Developers & AI Engineers:** To debug complex LLM applications, iterate faster, and ensure quality.\n",
      "*   **MLOps Teams:** To monitor LLM applications in production, manage costs, and track performance.\n",
      "*   **Researchers:** To experiment with different LLM architectures and evaluate their effectiveness systematically.\n",
      "\n",
      "In essence, LangSmith aims to bring robust MLOps practices to the rapidly evolving field of LLM application development, helping teams build reliable, performant, and maintainable AI products.\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    \"What is LangChain\",\n",
    "    \"What is LangGraph\",\n",
    "    \"What is LangSmith\"\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    result = llm_batch.invoke(q)\n",
    "    print(f\"\\nQuestion: {q}\")\n",
    "    print(f\"Answer: {result.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c438fce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
